{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "pdf_path = \"mcs2024.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the material lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_word_positions(pdf_path, 0)\n",
    "complexe_df = complexe_word(df, 2)\n",
    "\n",
    "complexe_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = extract_cols(complexe_df, 'x0', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['max_top'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"mcs2024.pdf\") as pdf:\n",
    "    im = pdf.pages[0].to_image(resolution=250)\n",
    "\n",
    "im.draw_line([(575, result_df['max_top'].min()), (44, result_df['max_top'].min())], )\n",
    "im.draw_line([(575, result_df['min_bottom'].max()), (44, result_df['min_bottom'].max())], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_list = complexe_df[(complexe_df['top'] > int(result_df['max_top'].min())) & (complexe_df['bottom'] < 630)]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/materials_list.json', 'r') as json_file:\n",
    "    materials_list = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match each material with its pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir le PDF\n",
    "elements = materials_list\n",
    "l = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    # Dictionnaire pour stocker les pages correspondant Ã  chaque Ã©lÃ©ment\n",
    "    pdf_sections = {material: {'title': pd.DataFrame(), 'pages': []} for material in materials_list}\n",
    "    \n",
    "    # Parcourir chaque page du PDF\n",
    "    for i in range(33, 209):\n",
    "\n",
    "        # page = pdf.pages[i]\n",
    "        largest_text = extract_largest_text(pdf_path, i)['text'].to_list()\n",
    "        # l += largest_text\n",
    "\n",
    "        for text in largest_text:\n",
    "            matched_element = match_element_in_text(materials_list, text)\n",
    "            if matched_element:\n",
    "                pdf_sections[matched_element]['title'] = pd.concat([pdf_sections[matched_element]['title'], extract_largest_text(pdf_path, i)], ignore_index=True)\n",
    "                pdf_sections[matched_element]['pages'].append(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_pages_elements = [material for material, details in pdf_sections.items() if not details['pages']]\n",
    "\n",
    "excluded_elements = {'Palladium'}\n",
    "empty_pages_elements = list(set(empty_pages_elements) - excluded_elements)\n",
    "\n",
    "empty_pages_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 1]\n",
    "\n",
    "one_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 3]\n",
    "\n",
    "three_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 4]\n",
    "\n",
    "four_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_sections['Sand and Gravel']['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{material_name: {\n",
    "'material_title': df(title, bounding box),\n",
    "'pages_num': [page1, page2],\n",
    "'pages_content': [dataframe1, dataframe2],\n",
    "  'remarks': [remark_df],\n",
    "  'tables': [[tables_1(table + bounding_box)], [tables_2(table + bounding_box)]]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf_sections['Abrasives']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base = extract_positions_for_elements(pdf_path, pdf_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material, data in scraping_base_bricolage.items():\n",
    "    # Ensure pages_content is not empty\n",
    "    if data['pages_content']:\n",
    "        # print(f\" â–¶ï¸ {data['pages_content'][0]}\")\n",
    "        remarks = extract_text_between_delimiters(data['pages_content'][0], pdf_path, data['pages_num'][0])\n",
    "        data['remarks'] = remarks\n",
    "    else:\n",
    "        print(f\" â–¶ï¸ No pages_content for material: {material}\")\n",
    "    \n",
    "    data['remarks'] = data['remarks'].drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['remarks'].columns, data['pages_content'][0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert serialized dictionaries back to DataFrames\n",
    "def convert_dict_to_df(d):\n",
    "    if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "        return pd.DataFrame(d)\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_dict_to_df(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "  scraping_base[material]['tables'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Suppress specific FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The 'method' keyword in Series.replace is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Calling float on a single element Series is deprecated\")\n",
    "\n",
    "mt = 3\n",
    "\n",
    "for material in list(scraping_base.keys()):\n",
    "    if isinstance(scraping_base[material]['tables'], pd.DataFrame):\n",
    "        scraping_base[material]['tables'] = scraping_base[material]['tables'].to_dict(orient='records')\n",
    "    else:\n",
    "        scraping_base[material]['tables'] = []\n",
    "\n",
    "    selected_ps, contents = scraping_base[material]['pages_num'], scraping_base[material]['pages_content']\n",
    "    i = 0\n",
    "\n",
    "    for selected_p in selected_ps:\n",
    "        content = contents[i]\n",
    "        i += 1\n",
    "        result = extract_table(selected_p - 1, content, mt)\n",
    "        if result is None or len(result[0]) == 0:\n",
    "            continue\n",
    "\n",
    "        list_of_table_df, list_of_bbox, page = result\n",
    "        table_dfs = table_to_df(list_of_table_df, list_of_bbox, page, pdf_path)\n",
    "        if not table_dfs:\n",
    "            continue\n",
    "\n",
    "        for table_df, bbox in zip(table_dfs, list_of_bbox):\n",
    "            table = table_df.sort_values('bottom')\n",
    "            scraping_base[material]['tables'].append((table.to_dict(orient='records'), bbox, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_dict(d):\n",
    "    if isinstance(d, pd.DataFrame):\n",
    "        return d.to_dict(orient='records')\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_df_to_dict(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_df_to_dict(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "serializable_scraping_base = convert_df_to_dict(scraping_base)\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open('json_files/scraping_base_with_tables.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(serializable_scraping_base, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert serialized dictionaries back to DataFrames\n",
    "def convert_dict_to_df(d):\n",
    "    if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "        return pd.DataFrame(d)\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_dict_to_df(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "    for i in range(len(scraping_base[material]['tables'])):\n",
    "        table, bbox, page = scraping_base[material]['tables'][i]\n",
    "        table = table.sort_values('bottom')\n",
    "        table = table.drop('bottom', axis=1)\n",
    "        scraping_base[material]['tables'][i][0] = table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('excel_output/all_extracted_tables.xlsx', engine='openpyxl') as writer:\n",
    "    for material, data in scraping_base.items():\n",
    "\n",
    "        # Write remarks and tables in the same sheet, but separated by columns\n",
    "        combined_sheet_name = f\"{material}_raw_info\"\n",
    "        \n",
    "        # Write remarks\n",
    "        remarks_df = data['remarks'][['text']]\n",
    "        remarks_df.columns = ['remark']\n",
    "        remarks_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # Write each table starting from a new column\n",
    "        start_col = len(remarks_df.columns) + 2  # Adding 2 for separation\n",
    "        for i, (table_df, bbox, page) in enumerate(data['tables']):\n",
    "            table_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=start_col, index=False)\n",
    "            # Increment start_col by the number of columns in the table + 2 for separation\n",
    "            start_col += len(table_df.columns) + 2\n",
    "\n",
    "print(\"Excel file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remarks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF visualisation V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf_path = 'pdf_output/mcs2024_data_all_tables.pdf'\n",
    "\n",
    "draw_rectangles_for_materials(pdf_path, output_pdf_path, scraping_base_bricolage)\n",
    "\n",
    "print(f\"Rectangles drawn for materials in {pdf_path} and saved to {output_pdf_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep useful table in pdf and excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_df(d):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            d[key] = convert_dict_to_df(value)\n",
    "        elif isinstance(value, list):\n",
    "            if all(isinstance(item, dict) for item in value):\n",
    "                d[key] = pd.DataFrame(value)\n",
    "            else:\n",
    "                d[key] = [convert_dict_to_df(item) if isinstance(item, dict) else item for item in value]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_contains_target_phrases(table, target_phrases):\n",
    "    return any(any(phrase.lower() in str(cell).lower() for cell in row) for row in table.values for phrase in target_phrases)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert dictionary to DataFrame where applicable\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target phrases\n",
    "target_phrases = ['World total (rounded)', 'World total']\n",
    "\n",
    "\n",
    "# Filter the tables\n",
    "for material in scraping_base.keys():\n",
    "    new_dfs = []\n",
    "    for table_data in scraping_base[material]['tables']:\n",
    "        table, bbox, page = table_data\n",
    "        df = pd.DataFrame(table)\n",
    "        # Check if any cell in the dataframe matches the target phrases\n",
    "        if df.apply(lambda row: any(match_element_in_text(target_phrases, str(cell)) for cell in row), axis=1).any():\n",
    "            new_dfs.append(table_data)\n",
    "\n",
    "        # if isinstance(table, pd.DataFrame) and table_contains_target_phrases(table, target_phrases):\n",
    "        #     filtered_tables.append(table_data)\n",
    "    \n",
    "    # Update the 'tables' key with the filtered tables\n",
    "    scraping_base[material]['tables'] = new_dfs     \n",
    "\n",
    "# Save the filtered data as a new JSON file\n",
    "with open('json_files/scraping_base_with_needed_tables.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(scraping_base, json_file, indent=2, default=lambda x: x.to_dict() if isinstance(x, pd.DataFrame) else str(x))\n",
    "\n",
    "print(\"Filtered JSON file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_needed_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "    if 'remarks' in scraping_base[material]:\n",
    "        remarks = scraping_base[material]['remarks']\n",
    "        remarks_df = pd.DataFrame(remarks)\n",
    "        scraping_base[material]['remarks'] = remarks_df\n",
    "\n",
    "    if 'tables' in scraping_base[material]:    \n",
    "        for i in range(len(scraping_base[material]['tables'])):\n",
    "            table, bbox, page = scraping_base[material]['tables'][i]\n",
    "            # Convert table to DataFrame\n",
    "            df = pd.DataFrame(table)\n",
    "            # Sort and drop 'bottom' column if it exists\n",
    "            if 'bottom' in df.columns:\n",
    "                df = df.sort_values('bottom')\n",
    "                df = df.drop('bottom', axis=1)\n",
    "            scraping_base[material]['tables'][i][0] = df\n",
    "\n",
    "with pd.ExcelWriter('excel_output/production_reserve_tables1.xlsx', engine='openpyxl') as writer:\n",
    "    for material, data in scraping_base.items():\n",
    "        # print(f\"ðŸŸ data: {data['remarks']}\")\n",
    "        # Write remarks and tables in the same sheet, but separated by columns\n",
    "        combined_sheet_name = f\"{material}_raw_info\"\n",
    "        \n",
    "        # Write remarks\n",
    "        remarks_df = data['remarks'][['text']]\n",
    "        # print('ðŸŸ remark_df:', remarks_df)\n",
    "        remarks_df.columns = ['remark']\n",
    "        remarks_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # Write each table starting from a new column\n",
    "        start_col = len(remarks_df.columns) + 2  # Adding 2 for separation\n",
    "        for i, (table_df, bbox, page) in enumerate(data['tables']):\n",
    "            table_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=start_col, index=False)\n",
    "            # Increment start_col by the number of columns in the table + 2 for separation\n",
    "            start_col += len(table_df.columns) + 2\n",
    "\n",
    "print(\"Excel file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_needed_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "# with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)\n",
    "\n",
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}\n",
    "\n",
    "output_pdf_path = 'pdf_output/mcs2024_data_prod_reserve_tables.pdf'\n",
    "\n",
    "draw_rectangles_for_materials(pdf_path, output_pdf_path, scraping_base_bricolage)\n",
    "\n",
    "print(f\"Rectangles drawn for materials in {pdf_path} and saved to {output_pdf_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remark interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_input = 'excel_output\\production_reserve_tables1_updated_manualy.xlsx'\n",
    "\n",
    "import json\n",
    "\n",
    "with open('json_files/scraping_base.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_remarks = {material: {'remarks': content['remarks'][['text']]} for material, content in scraping_base.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_conversion_df = pd.read_csv('sub_materials_database.csv\\sub_materials_database_metric_conversion_factor.csv')\n",
    "\n",
    "chem_compo_df = pd.read_csv('sub_materials_database.csv\\sub_materials_database_sub_materials_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversion_factor(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'Not applicable' in value:\n",
    "            return value\n",
    "        # Extract numeric values using regex\n",
    "        match = re.search(r'([-+]?\\d*\\.\\d+|\\d+)', value)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "    return value\n",
    "\n",
    "metric_conversion_df['conversion_factor'] = metric_conversion_df['conversion_factor'].apply(process_conversion_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_conversion_df[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set all 'final_unit' values to 't'\n",
    "metric_conversion_df['final_unit'] = 't'\n",
    "\n",
    "# Then, update 'final_unit' to 'karat' only for 'Karat (gold)'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Karat (gold)', 'final_unit'] = 'karat'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Mcf (1,000 cubic feet)', 'final_unit'] = 'cubic feet'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Psia', 'final_unit'] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_conversion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Copy the original Excel file to create an updated version\n",
    "excel_output = 'excel_output/produc_reserve_remarkinterpre.xlsx'\n",
    "shutil.copyfile(excel_input, excel_output)\n",
    "\n",
    "# Open the Excel file for updating\n",
    "with pd.ExcelWriter(excel_output, mode='a', if_sheet_exists='replace') as writer:\n",
    "    for material in material_remarks.keys():\n",
    "        # Convert the remarks to a DataFrame\n",
    "        material_sample = pd.DataFrame(material_remarks[material]['remarks'], columns=['text'])\n",
    "\n",
    "        # List of sub-material names from the reference_db\n",
    "        materials_list = chem_compo_df['sub_material_name'].tolist()\n",
    "\n",
    "        # List of metric units from the metric_conversion_df\n",
    "        metrics_list = metric_conversion_df['Metric Mentioned'].tolist()\n",
    "\n",
    "        # Apply the functions to the 'text' column of material_sample\n",
    "        material_sample['chemical_composition'] = material_sample['text'].apply(\n",
    "            lambda text: find_all_chemical_compositions(text, materials_list, chem_compo_df))\n",
    "        material_sample['metric_conv_factor'] = material_sample['text'].apply(\n",
    "            lambda text: find_metric_conversion_factor(text, metrics_list, metric_conversion_df))\n",
    "\n",
    "        # Read the corresponding sheet from the original Excel file\n",
    "        sheet_name = f\"{material}_raw_info\"\n",
    "        original_df = pd.read_excel(excel_input, sheet_name=sheet_name)\n",
    "\n",
    "        # Ensure the new DataFrame (material_sample) has the same index as the original_df\n",
    "        material_sample = material_sample.reindex(original_df.index)\n",
    "\n",
    "        # Insert the new columns next to the 'remark' column\n",
    "        original_df.insert(1, 'chemical_composition', material_sample['chemical_composition'])\n",
    "        original_df.insert(2, 'metric_conv_factor', material_sample['metric_conv_factor'])\n",
    "\n",
    "        # Write the updated sheet back to the Excel file\n",
    "        original_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## production of each material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_input = 'excel_output/produc_reserve_remarkinterpre.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = pd.ExcelFile(excel_input)\n",
    "\n",
    "# List to store dictionaries of sheet names and DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through all sheet names\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    \n",
    "    # Create a dictionary with sheet name and DataFrame\n",
    "    sheet_dict = {\n",
    "        f'sheet_name': sheet_name,\n",
    "        f'df': df\n",
    "    }\n",
    "    \n",
    "    # Append the dictionary to the list\n",
    "    dfs.append(sheet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_exclude = ['Reserves', 'Capacity', 'reserves', 'capacity']  # Add or remove elements as needed\n",
    "\n",
    "def contains_excluded_word(column, excluded_words):\n",
    "    def check_cell(cell):\n",
    "        cell_str = str(cell).lower()\n",
    "        return any(word.lower() in cell_str for word in excluded_words)\n",
    "    \n",
    "    return column.apply(check_cell).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "def match_composition(input_name, reference_df):\n",
    "    input_name = str(input_name).lower()\n",
    "    best_match = None\n",
    "    best_ratio = 0\n",
    "    for material in reference_df['sub_material_name']:\n",
    "        ratio = fuzz.partial_ratio(input_name, str(material).lower())\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = material\n",
    "    if best_ratio < 80:\n",
    "        return None\n",
    "    composition = reference_df.loc[reference_df['sub_material_name'] == best_match, 'chemical_composition'].iloc[0]\n",
    "    return composition\n",
    "\n",
    "def clean_numeric(val):\n",
    "    if isinstance(val, str):\n",
    "        cleaned = re.sub(r'[^\\d.]+', '', val)\n",
    "        return float(cleaned) if cleaned else np.nan\n",
    "    return val\n",
    "\n",
    "def convert_year_to_float(year):\n",
    "    if isinstance(year, str):\n",
    "        year = year.strip().lower()\n",
    "        if year.endswith('e'):\n",
    "            return float(year[:-1])\n",
    "        try:\n",
    "            return float(year)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return int(year) if pd.notnull(year) else np.nan\n",
    "\n",
    "def process_column(df, text_df, title_row, index, item, years):\n",
    "    for year in years:\n",
    "        year_float = convert_year_to_float(year)\n",
    "        if pd.notna(year_float):\n",
    "            chem_comp_dict = item['chem_comp']\n",
    "            if chem_comp_dict:\n",
    "                for elem, percentage in chem_comp_dict.items():\n",
    "                    new_col_name = f\"{elem}_{year_float}\"\n",
    "                    try:\n",
    "                        numeric_col = text_df.iloc[2:][title_row[index]].apply(clean_numeric)\n",
    "                        result = numeric_col * percentage\n",
    "                        result = result.replace([np.inf, -np.inf], np.nan)\n",
    "                        if new_col_name in df.columns:\n",
    "                            df[new_col_name] += result\n",
    "                        else:\n",
    "                            df[new_col_name] = pd.Series(index=df.index)\n",
    "                            df.loc[df.index[2:], new_col_name] = result.values\n",
    "                        print(f\"Created/Updated column: {new_col_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"ðŸ”´Error processing column {title_row[index]}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"ðŸ”´ No valid chemical composition for {item['material']}. Keeping original data.\")\n",
    "\n",
    "def process_varied_composition(df, text_df, title_row, sheet_material, first_row, second_row):\n",
    "    for index, col in enumerate(title_row):\n",
    "        if text_df[col].iloc[2:].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna().any():\n",
    "            years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "            for year in years:\n",
    "                year_float = convert_year_to_float(year)\n",
    "                if pd.notna(year_float):\n",
    "                    new_col_name = f\"{sheet_material}_{year_float}\"\n",
    "                    try:\n",
    "                        numeric_col = text_df.iloc[2:][col].apply(clean_numeric)\n",
    "                        if new_col_name in df.columns:\n",
    "                            df[new_col_name] += numeric_col\n",
    "                        else:\n",
    "                            df[new_col_name] = pd.Series(index=df.index)\n",
    "                            df.loc[df.index[2:], new_col_name] = numeric_col.values\n",
    "                        print(f\"Created/Updated column: {new_col_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"ðŸ”´Error processing column {col}: {str(e)}\")\n",
    "\n",
    "materials_missing_comp = []\n",
    "for i in range(0, len(dfs)):\n",
    "    sheet_name = dfs[i]['sheet_name']\n",
    "    df = dfs[i]['df'].copy()\n",
    "    print(\"SHEET NAME : \", sheet_name)\n",
    "\n",
    "    original_text_columns = [col for col in df.columns if col.startswith('text')]\n",
    "    columns_to_keep = [col for col in df.columns if not contains_excluded_word(df[col], words_to_exclude)]\n",
    "    df = df[columns_to_keep]\n",
    "    remaining_text_columns = [col for col in df.columns if col.startswith('text')]\n",
    "\n",
    "    if remaining_text_columns:\n",
    "        text_df = df[remaining_text_columns]\n",
    "        first_row = text_df.iloc[0] if not text_df.empty else pd.Series()\n",
    "        second_row = text_df.iloc[1] if len(text_df) > 1 else pd.Series()\n",
    "        title_row = list(text_df.columns)\n",
    "        materials = list(pd.unique(first_row.dropna().values))\n",
    "\n",
    "        list_chem_comp = []\n",
    "        for material in materials:\n",
    "            chem_comp = match_composition(material, chem_compo_df)\n",
    "            list_chem_comp.append({\"material\": material, \"chem_comp\": chem_comp})\n",
    "\n",
    "        if all(item['chem_comp'] is None for item in list_chem_comp):\n",
    "            material = sheet_name.replace('_raw_info', '')\n",
    "            list_chem_comp = [{\"material\": material, \"chem_comp\": match_composition(material, chem_compo_df)}]\n",
    "\n",
    "        if all(item['chem_comp'] is None for item in list_chem_comp):\n",
    "            materials_missing_comp.append(sheet_name.replace('_raw_info', ''))\n",
    "            print(\"ðŸ§¿ no match\")\n",
    "        else:\n",
    "            list_chem_comp = convert_string_to_dict(list_chem_comp)\n",
    "            df['chemical_composition'] = str(list_chem_comp)\n",
    "\n",
    "            if list_chem_comp[0]['chem_comp'] != 'Varied':\n",
    "                sheet_material = sheet_name.replace('_raw_info', '')\n",
    "                is_sheet_material = any(item['material'] == sheet_material for item in list_chem_comp)\n",
    "\n",
    "                if is_sheet_material:\n",
    "                    for index, col in enumerate(title_row):\n",
    "                        if text_df[col].iloc[2:].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna().any():\n",
    "                            item = next((item for item in list_chem_comp if item['material'] == sheet_material), None)\n",
    "                            if item:\n",
    "                                years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "                                process_column(df, text_df, title_row, index, item, years)\n",
    "                else:\n",
    "                    for index, col in enumerate(first_row):\n",
    "                        for item in list_chem_comp:\n",
    "                            if col == item['material']:\n",
    "                                years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "                                process_column(df, text_df, title_row, index, item, years)\n",
    "                print('ðŸŸ¢ not varied')\n",
    "            else:\n",
    "                print('ðŸŸ¢ varied')\n",
    "                sheet_material = sheet_name.replace('_raw_info', '')\n",
    "                process_varied_composition(df, text_df, title_row, sheet_material, first_row, second_row)\n",
    "                print(\"ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢ðŸŸ¢ ===> \", sheet_material)\n",
    "    else:\n",
    "        print(f\"No text columns remaining for sheet: {sheet_name}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    dfs[i]['df'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_missing_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dfs list by sheet_name\n",
    "dfs = sorted(dfs, key=lambda x: x['sheet_name'])\n",
    "\n",
    "# Save all DataFrames to a single Excel file\n",
    "output_file = 'excel_output/production_each_material.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for df_dict in dfs:\n",
    "        sheet_name = df_dict['sheet_name']\n",
    "        df = df_dict['df']\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"All processed DataFrames have been saved to {output_file} with sheets sorted alphabetically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_post_text_columns(columns):\n",
    "    last_text_index = max([i for i, col in enumerate(columns) if col.startswith('text')], default=-1)\n",
    "    return columns[last_text_index + 1:] if last_text_index != -1 else []\n",
    "\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "res_dfs = []\n",
    "\n",
    "for i in range(0, len(dfs)-13):\n",
    "    sheet_name = dfs[i]['sheet_name']\n",
    "    df = dfs[i]['df'].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    print(\"SHEET NAME : \", sheet_name)\n",
    "    \n",
    "    # Safely access conv_factor\n",
    "    conv_factor = df['metric_conv_factor'].iloc[0] if 'metric_conv_factor' in df.columns else None\n",
    "    cols = list(df.columns)\n",
    "    post_text_columns = detect_post_text_columns(cols)\n",
    "\n",
    "    if post_text_columns:\n",
    "        data = df[['text'] + post_text_columns]\n",
    "        data = data.dropna(how='all')\n",
    "\n",
    "        if conv_factor is not None and not isinstance(conv_factor, str):\n",
    "            try:\n",
    "                conv_factor = float(conv_factor)\n",
    "                for col in post_text_columns:\n",
    "                    data[col] = data[col].apply(lambda x: x * conv_factor if is_numeric(x) else x)\n",
    "                print(f\"Applied conversion factor {conv_factor} to numerical values in {sheet_name}\")\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert {conv_factor} to float in {sheet_name}\")\n",
    "        \n",
    "        res_dfs.append(data)\n",
    "    else:\n",
    "        print(f\"No post-text columns found in {sheet_name}\")\n",
    "\n",
    "# Merge all data(s) in res_dfs\n",
    "if res_dfs:\n",
    "    merged_df = pd.concat(res_dfs, keys=[f\"Sheet_{i}\" for i in range(len(res_dfs))], axis=0)\n",
    "    print(\"All dataframes merged successfully\")\n",
    "else:\n",
    "    merged_df = pd.DataFrame()\n",
    "    print(\"No dataframes to merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns = merged_df.columns.str.replace(r'\\.0$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text'] = merged_df['text'].str.lower()\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First replace 'world total' with a common string\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'^world total.*', 'world total', regex=True, case=False)\n",
    "\n",
    "# Remove any text between parentheses and strip leading/trailing spaces\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text'] = merged_df['text'].str.split(',', n=1).str[0].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = ['natural:', 'other', 'other countries']\n",
    "\n",
    "# Remove rows where 'text' equals any value in the outliers list\n",
    "merged_df = merged_df[~merged_df['text'].isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_summed = merged_df.groupby('text', as_index=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = merged_df_summed.groupby(merged_df_summed.columns, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# List of standard country names\n",
    "countries = [\n",
    "    \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\", \"Argentina\", \"Armenia\", \"Australia\", \n",
    "    \"Austria\", \"Azerbaijan\", \"Bahamas\", \"Bahrain\", \"Bangladesh\", \"Barbados\", \"Belarus\", \"Belgium\", \"Belize\", \"Benin\", \n",
    "    \"Bhutan\", \"Bolivia\", \"Bosnia and Herzegovina\", \"Botswana\", \"Brazil\", \"Brunei\", \"Bulgaria\", \"Burkina Faso\", \"Burundi\", \n",
    "    \"Cabo Verde\", \"Cambodia\", \"Cameroon\", \"Canada\", \"Central African Republic\", \"Chad\", \"Chile\", \"China\", \"Colombia\", \n",
    "    \"Comoros\", \"Congo\", \"Costa Rica\", \"Croatia\", \"Cuba\", \"Cyprus\", \"Czech Republic\", \"Democratic Republic of the Congo\", \n",
    "    \"Denmark\", \"Djibouti\", \"Dominica\", \"Dominican Republic\", \"Ecuador\", \"Egypt\", \"El Salvador\", \"Equatorial Guinea\", \n",
    "    \"Eritrea\", \"Estonia\", \"Eswatini\", \"Ethiopia\", \"Fiji\", \"Finland\", \"France\", \"Gabon\", \"Gambia\", \"Georgia\", \"Germany\", \n",
    "    \"Ghana\", \"Greece\", \"Grenada\", \"Guatemala\", \"Guinea\", \"Guinea-Bissau\", \"Guyana\", \"Haiti\", \"Honduras\", \"Hungary\", \n",
    "    \"Iceland\", \"India\", \"Indonesia\", \"Iran\", \"Iraq\", \"Ireland\", \"Israel\", \"Italy\", \"Ivory Coast\", \"Jamaica\", \"Japan\", \n",
    "    \"Jordan\", \"Kazakhstan\", \"Kenya\", \"Kiribati\", \"Kuwait\", \"Kyrgyzstan\", \"Laos\", \"Latvia\", \"Lebanon\", \"Lesotho\", \"Liberia\", \n",
    "    \"Libya\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Madagascar\", \"Malawi\", \"Malaysia\", \"Maldives\", \"Mali\", \"Malta\", \n",
    "    \"Marshall Islands\", \"Mauritania\", \"Mauritius\", \"Mexico\", \"Micronesia\", \"Moldova\", \"Monaco\", \"Mongolia\", \"Montenegro\", \n",
    "    \"Morocco\", \"Mozambique\", \"Myanmar\", \"Namibia\", \"Nauru\", \"Nepal\", \"Netherlands\", \"New Zealand\", \"Nicaragua\", \"Niger\", \n",
    "    \"Nigeria\", \"North Korea\", \"North Macedonia\", \"Norway\", \"Oman\", \"Pakistan\", \"Palau\", \"Palestine\", \"Panama\", \n",
    "    \"Papua New Guinea\", \"Paraguay\", \"Peru\", \"Philippines\", \"Poland\", \"Portugal\", \"Qatar\", \"Romania\", \"Russia\", \"Rwanda\", \n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\", \"Samoa\", \"San Marino\", \n",
    "    \"Sao Tome and Principe\", \"Saudi Arabia\", \"Senegal\", \"Serbia\", \"Seychelles\", \"Sierra Leone\", \"Singapore\", \"Slovakia\", \n",
    "    \"Slovenia\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Korea\", \"South Sudan\", \"Spain\", \"Sri Lanka\", \"Sudan\", \n",
    "    \"Suriname\", \"Sweden\", \"Switzerland\", \"Syria\", \"Taiwan\", \"Tajikistan\", \"Tanzania\", \"Thailand\", \"Timor-Leste\", \"Togo\", \n",
    "    \"Tonga\", \"Trinidad and Tobago\", \"Tunisia\", \"Turkey\", \"Turkmenistan\", \"Tuvalu\", \"Uganda\", \"Ukraine\", \n",
    "    \"United Arab Emirates\", \"United Kingdom\", \"United States\", \"Uruguay\", \"Uzbekistan\", \"Vanuatu\", \"Vatican City\", \n",
    "    \"Venezuela\", \"Vietnam\", \"Yemen\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "def match_country(country, choices, cutoff=80):\n",
    "    match = process.extractOne(country, choices, score_cutoff=cutoff)\n",
    "    return match[0] if match else country\n",
    "\n",
    "# Apply the matching function to the 'text' column\n",
    "out_df['text'] = out_df['text'].apply(lambda x: match_country(x, countries) if x != 'world total' and x != 'calculated world total' else x)\n",
    "\n",
    "# Group by the standardized names and sum the numeric columns\n",
    "out_df = out_df.groupby('text', as_index=False).sum()\n",
    "\n",
    "# Recalculate the world total\n",
    "world_total_row = out_df[out_df['text'] == 'world total']\n",
    "other_rows = out_df[out_df['text'] != 'world total']\n",
    "\n",
    "calculated_world_total = other_rows.select_dtypes(include=[np.number]).sum().to_frame().T\n",
    "calculated_world_total['text'] = 'calculated world total'\n",
    "\n",
    "# Combine the rows\n",
    "out_df = pd.concat([other_rows, calculated_world_total], ignore_index=True)\n",
    "\n",
    "# Sort the columns to ensure they are in the correct order\n",
    "out_df = out_df.reindex(columns=['text'] + sorted([col for col in out_df.columns if col != 'text']))\n",
    "\n",
    "# Reset the index\n",
    "out_df = out_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the columns list with 'text' at the beginning\n",
    "columns_order = ['text'] + [col for col in out_df.columns if col != 'text']\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "out_df = out_df[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out_df into 2022_df and 2023_df\n",
    "columns_2022 = ['text'] + [col for col in out_df.columns if col.endswith('2022')]\n",
    "columns_2023 = ['text'] + [col for col in out_df.columns if col.endswith('2023')]\n",
    "\n",
    "# Create DataFrames for each year\n",
    "df_22 = out_df[columns_2022]\n",
    "df_23 = out_df[columns_2023]\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "with pd.ExcelWriter('final_table.xlsx', engine='openpyxl') as writer:\n",
    "    df_22.to_excel(writer, sheet_name='2022 Data', index=False)\n",
    "    df_23.to_excel(writer, sheet_name='2023 Data', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
