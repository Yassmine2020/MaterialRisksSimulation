{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "pdf_path = \"mcs2024.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the material lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_word_positions(pdf_path, 0)\n",
    "complexe_df = complexe_word(df, 2)\n",
    "\n",
    "complexe_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = extract_cols(complexe_df, 'x0', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['max_top'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"mcs2024.pdf\") as pdf:\n",
    "    im = pdf.pages[0].to_image(resolution=250)\n",
    "\n",
    "im.draw_line([(575, result_df['max_top'].min()), (44, result_df['max_top'].min())], )\n",
    "im.draw_line([(575, result_df['min_bottom'].max()), (44, result_df['min_bottom'].max())], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_list = complexe_df[(complexe_df['top'] > int(result_df['max_top'].min())) & (complexe_df['bottom'] < 630)]['text'].tolist()\n",
    "\n",
    "len(materials_list) #游릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files\\materials_list.json', 'w') as json_file:\n",
    "    json.dump(materials_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/materials_list.json', 'r') as json_file:\n",
    "    materials_list = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_list #游릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match each material with its pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir le PDF\n",
    "elements = materials_list\n",
    "l = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    # Dictionnaire pour stocker les pages correspondant  chaque 칠l칠ment\n",
    "    pdf_sections = {material: {'title': pd.DataFrame(), 'pages': []} for material in materials_list}\n",
    "    \n",
    "    # Parcourir chaque page du PDF\n",
    "    for i in range(33, 209):\n",
    "\n",
    "        # page = pdf.pages[i]\n",
    "        largest_text = extract_largest_text(pdf_path, i)['text'].to_list()\n",
    "        # l += largest_text\n",
    "        # print(f'郊윒잺 Largest_text: {largest_text}')\n",
    "\n",
    "        for text in largest_text:\n",
    "            matched_element = match_element_in_text(materials_list, text)\n",
    "            if matched_element:\n",
    "                pdf_sections[matched_element]['title'] = pd.concat([pdf_sections[matched_element]['title'], extract_largest_text(pdf_path, i)], ignore_index=True)\n",
    "                pdf_sections[matched_element]['pages'].append(i + 1)\n",
    "\n",
    "# print(f'郊윒잺 Largest_text len: {len(l)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_pages_elements = [material for material, details in pdf_sections.items() if not details['pages']]\n",
    "\n",
    "excluded_elements = {'Palladium'}\n",
    "empty_pages_elements = list(set(empty_pages_elements) - excluded_elements)\n",
    "\n",
    "empty_pages_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 1]\n",
    "\n",
    "one_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 3]\n",
    "\n",
    "three_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_page_elements = [material for material, details in pdf_sections.items() if len(details['pages']) == 4]\n",
    "\n",
    "four_page_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_sections['Sand and Gravel']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df_to_dict(d):\n",
    "#     if isinstance(d, pd.DataFrame):\n",
    "#         return d.to_dict(orient='records')\n",
    "#     elif isinstance(d, list):\n",
    "#         return [convert_df_to_dict(i) for i in d]\n",
    "#     elif isinstance(d, dict):\n",
    "#         return {k: convert_df_to_dict(v) for k, v in d.items()}\n",
    "#     else:\n",
    "#         return d\n",
    "\n",
    "# serializable_pdf_sections = convert_df_to_dict(pdf_sections)\n",
    "\n",
    "# # Save the dictionary to a JSON file\n",
    "# with open('json_files/pdf_sections.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(serializable_pdf_sections, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('json_files/pdf_sections.json', 'r', encoding='utf-8') as json_file:\n",
    "#     loaded_pdf_sections = json.load(json_file)\n",
    "\n",
    "# # Convert serialized dictionaries back to DataFrames\n",
    "# def convert_dict_to_df(d):\n",
    "#     if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "#         return pd.DataFrame(d)\n",
    "#     elif isinstance(d, list):\n",
    "#         return [convert_dict_to_df(i) for i in d]\n",
    "#     elif isinstance(d, dict):\n",
    "#         return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "#     else:\n",
    "#         return d\n",
    "\n",
    "# pdf_sections = convert_dict_to_df(loaded_pdf_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_sections   #游 sauf for Zirconium!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary: \n",
    "{material_name: {\n",
    "'material_title': df(title, bounding box),\n",
    "'pages_num': [page1, page2],\n",
    "'pages_content': [dataframe1, dataframe2],\n",
    "  'remarks': [remark_df],\n",
    "  'tables': [[tables_1(table + bounding_box)], [tables_2(table + bounding_box)]]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_sections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdf_sections['Abrasives']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base = extract_positions_for_elements(pdf_path, pdf_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base['Abrasives'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base['Abrasives']['remarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df_to_dict(d):\n",
    "#     if isinstance(d, pd.DataFrame):\n",
    "#         return d.to_dict(orient='records')\n",
    "#     elif isinstance(d, list):\n",
    "#         return [convert_df_to_dict(i) for i in d]\n",
    "#     elif isinstance(d, dict):\n",
    "#         return {k: convert_df_to_dict(v) for k, v in d.items()}\n",
    "#     else:\n",
    "#         return d\n",
    "\n",
    "# serializable_scraping_base = convert_df_to_dict(scraping_base)\n",
    "\n",
    "# # Save the dictionary to a JSON file\n",
    "# with open('json_files/scraping_base.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(serializable_scraping_base, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('json_files/scraping_base.json', 'r', encoding='utf-8') as json_file:\n",
    "#     loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# # Convert serialized dictionaries back to DataFrames\n",
    "# def convert_dict_to_df(d):\n",
    "#     if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "#         return pd.DataFrame(d)\n",
    "#     elif isinstance(d, list):\n",
    "#         return [convert_dict_to_df(i) for i in d]\n",
    "#     elif isinstance(d, dict):\n",
    "#         return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "#     else:\n",
    "#         return d\n",
    "\n",
    "# scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base['Abrasives']  #游릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage['Gallium']['remarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for material in list(scraping_base_bricolage.keys()):\n",
    "#   scraping_base_bricolage[material]['remarks'] = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material, data in scraping_base_bricolage.items():\n",
    "    # Ensure pages_content is not empty\n",
    "    if data['pages_content']:\n",
    "        # print(f\" 郊윒잺 {data['pages_content'][0]}\")\n",
    "        remarks = extract_text_between_delimiters(data['pages_content'][0], pdf_path, data['pages_num'][0])\n",
    "        data['remarks'] = remarks\n",
    "    else:\n",
    "        print(f\" 郊윒잺 No pages_content for material: {material}\")\n",
    "    \n",
    "    data['remarks'] = data['remarks'].drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['remarks'].columns, data['pages_content'][0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage['Platinum']['remarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df_to_dict(d):\n",
    "#     if isinstance(d, pd.DataFrame):\n",
    "#         return d.to_dict(orient='records')\n",
    "#     elif isinstance(d, list):\n",
    "#         return [convert_df_to_dict(i) for i in d]\n",
    "#     elif isinstance(d, dict):\n",
    "#         return {k: convert_df_to_dict(v) for k, v in d.items()}\n",
    "#     else:\n",
    "#         return d\n",
    "\n",
    "# serializable_scraping_base = convert_df_to_dict(scraping_base_bricolage)\n",
    "\n",
    "# # Save the dictionary to a JSON file\n",
    "# with open('json_files/scraping_base.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(serializable_scraping_base, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "pdf_path = \"mcs2024.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert serialized dictionaries back to DataFrames\n",
    "def convert_dict_to_df(d):\n",
    "    if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "        return pd.DataFrame(d)\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_dict_to_df(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "  scraping_base[material]['tables'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Suppress specific FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The 'method' keyword in Series.replace is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Calling float on a single element Series is deprecated\")\n",
    "\n",
    "mt = 3\n",
    "\n",
    "for material in list(scraping_base.keys()):\n",
    "    if isinstance(scraping_base[material]['tables'], pd.DataFrame):\n",
    "        scraping_base[material]['tables'] = scraping_base[material]['tables'].to_dict(orient='records')\n",
    "    else:\n",
    "        scraping_base[material]['tables'] = []\n",
    "\n",
    "    selected_ps, contents = scraping_base[material]['pages_num'], scraping_base[material]['pages_content']\n",
    "    i = 0\n",
    "\n",
    "    for selected_p in selected_ps:\n",
    "        content = contents[i]\n",
    "        i += 1\n",
    "        result = extract_table(selected_p - 1, content, mt)\n",
    "        if result is None or len(result[0]) == 0:\n",
    "            continue\n",
    "\n",
    "        list_of_table_df, list_of_bbox, page = result\n",
    "        table_dfs = table_to_df(list_of_table_df, list_of_bbox, page, pdf_path)\n",
    "        if not table_dfs:\n",
    "            continue\n",
    "\n",
    "        for table_df, bbox in zip(table_dfs, list_of_bbox):\n",
    "            table = table_df.sort_values('bottom')\n",
    "            scraping_base[material]['tables'].append((table.to_dict(orient='records'), bbox, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# material_test = ['Arsenic', 'Bismuth', 'Cadmium']\n",
    "\n",
    "# material_test = ['Boron', 'Iron Ore', 'Strontium']\n",
    "\n",
    "# material_test = ['Sand and Gravel', 'Stone']\n",
    "\n",
    "# material_test = ['Chromium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import warnings\n",
    "\n",
    "# # Suppress SettingWithCopyWarning\n",
    "# pd.options.mode.chained_assignment = None\n",
    "\n",
    "# # Suppress specific FutureWarnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The 'method' keyword in Series.replace is deprecated\")\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Calling float on a single element Series is deprecated\")\n",
    "\n",
    "# # Your existing code\n",
    "# for material in material_test:\n",
    "#     if isinstance(scraping_base[material]['tables'], pd.DataFrame):\n",
    "#         scraping_base[material]['tables'] = scraping_base[material]['tables'].to_dict(orient='records')\n",
    "#     else:\n",
    "#         scraping_base[material]['tables'] = []\n",
    "\n",
    "#     selected_ps, contents = scraping_base[material]['pages_num'], scraping_base[material]['pages_content']\n",
    "#     i = 0\n",
    "\n",
    "#     for selected_p in selected_ps:\n",
    "#         content = contents[i]\n",
    "#         i += 1\n",
    "#         result = extract_table(selected_p - 1, content, 2)\n",
    "#         if result is None or len(result[0]) == 0:\n",
    "#             continue\n",
    "\n",
    "#         list_of_table_df, list_of_bbox, page = result\n",
    "#         table_dfs, combined_words = table_to_df(list_of_table_df, list_of_bbox, page, pdf_path)\n",
    "#         if not table_dfs:\n",
    "#             continue\n",
    "\n",
    "#         for table_df, bbox in zip(table_dfs, list_of_bbox):\n",
    "#             table = table_df.sort_values('bottom')\n",
    "#             # print(table)\n",
    "#             # print(\"*\"*40)\n",
    "#             # print()\n",
    "#             scraping_base[material]['tables'].append((table.to_dict(orient='records'), bbox, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[0][0]['text'].to_list()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_dict(d):\n",
    "    if isinstance(d, pd.DataFrame):\n",
    "        return d.to_dict(orient='records')\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_df_to_dict(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_df_to_dict(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "serializable_scraping_base = convert_df_to_dict(scraping_base)\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open('json_files/scraping_base_with_tables.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(serializable_scraping_base, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert serialized dictionaries back to DataFrames\n",
    "def convert_dict_to_df(d):\n",
    "    if isinstance(d, list) and all(isinstance(i, dict) for i in d):\n",
    "        return pd.DataFrame(d)\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_dict_to_df(i) for i in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: convert_dict_to_df(v) for k, v in d.items()}\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "    for i in range(len(scraping_base[material]['tables'])):\n",
    "        table, bbox, page = scraping_base[material]['tables'][i]\n",
    "        table = table.sort_values('bottom')\n",
    "        table = table.drop('bottom', axis=1)\n",
    "        scraping_base[material]['tables'][i][0] = table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('excel_output/all_extracted_tables.xlsx', engine='openpyxl') as writer:\n",
    "    for material, data in scraping_base.items():\n",
    "\n",
    "        # Write remarks and tables in the same sheet, but separated by columns\n",
    "        combined_sheet_name = f\"{material}_raw_info\"\n",
    "        \n",
    "        # Write remarks\n",
    "        remarks_df = data['remarks'][['text']]\n",
    "        remarks_df.columns = ['remark']\n",
    "        remarks_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # Write each table starting from a new column\n",
    "        start_col = len(remarks_df.columns) + 2  # Adding 2 for separation\n",
    "        for i, (table_df, bbox, page) in enumerate(data['tables']):\n",
    "            table_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=start_col, index=False)\n",
    "            # Increment start_col by the number of columns in the table + 2 for separation\n",
    "            start_col += len(table_df.columns) + 2\n",
    "\n",
    "print(\"Excel file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remarks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF visualisation V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf_path = 'pdf_output/mcs2024_data_all_tables.pdf'\n",
    "\n",
    "draw_rectangles_for_materials(pdf_path, output_pdf_path, scraping_base_bricolage)\n",
    "\n",
    "print(f\"Rectangles drawn for materials in {pdf_path} and saved to {output_pdf_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep useful table in pdf and excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_df(d):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            d[key] = convert_dict_to_df(value)\n",
    "        elif isinstance(value, list):\n",
    "            if all(isinstance(item, dict) for item in value):\n",
    "                d[key] = pd.DataFrame(value)\n",
    "            else:\n",
    "                d[key] = [convert_dict_to_df(item) if isinstance(item, dict) else item for item in value]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_contains_target_phrases(table, target_phrases):\n",
    "    return any(any(phrase.lower() in str(cell).lower() for cell in row) for row in table.values for phrase in target_phrases)\n",
    "\n",
    "# Load the JSON file\n",
    "with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "# Convert dictionary to DataFrame where applicable\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target phrases\n",
    "target_phrases = ['World total (rounded)', 'World total']\n",
    "\n",
    "\n",
    "# Filter the tables\n",
    "for material in scraping_base.keys():\n",
    "    new_dfs = []\n",
    "    for table_data in scraping_base[material]['tables']:\n",
    "        table, bbox, page = table_data\n",
    "        df = pd.DataFrame(table)\n",
    "        # Check if any cell in the dataframe matches the target phrases\n",
    "        if df.apply(lambda row: any(match_element_in_text(target_phrases, str(cell)) for cell in row), axis=1).any():\n",
    "            new_dfs.append(table_data)\n",
    "\n",
    "        # if isinstance(table, pd.DataFrame) and table_contains_target_phrases(table, target_phrases):\n",
    "        #     filtered_tables.append(table_data)\n",
    "    \n",
    "    # Update the 'tables' key with the filtered tables\n",
    "    scraping_base[material]['tables'] = new_dfs     \n",
    "\n",
    "# Save the filtered data as a new JSON file\n",
    "with open('json_files/scraping_base_with_needed_tables.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(scraping_base, json_file, indent=2, default=lambda x: x.to_dict() if isinstance(x, pd.DataFrame) else str(x))\n",
    "\n",
    "print(\"Filtered JSON file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_needed_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for material in list(scraping_base.keys()):\n",
    "    if 'remarks' in scraping_base[material]:\n",
    "        remarks = scraping_base[material]['remarks']\n",
    "        remarks_df = pd.DataFrame(remarks)\n",
    "        scraping_base[material]['remarks'] = remarks_df\n",
    "\n",
    "    if 'tables' in scraping_base[material]:    \n",
    "        for i in range(len(scraping_base[material]['tables'])):\n",
    "            table, bbox, page = scraping_base[material]['tables'][i]\n",
    "            # Convert table to DataFrame\n",
    "            df = pd.DataFrame(table)\n",
    "            # Sort and drop 'bottom' column if it exists\n",
    "            if 'bottom' in df.columns:\n",
    "                df = df.sort_values('bottom')\n",
    "                df = df.drop('bottom', axis=1)\n",
    "            scraping_base[material]['tables'][i][0] = df\n",
    "\n",
    "with pd.ExcelWriter('excel_output/production_reserve_tables1.xlsx', engine='openpyxl') as writer:\n",
    "    for material, data in scraping_base.items():\n",
    "        # print(f\"游맋ata: {data['remarks']}\")\n",
    "        # Write remarks and tables in the same sheet, but separated by columns\n",
    "        combined_sheet_name = f\"{material}_raw_info\"\n",
    "        \n",
    "        # Write remarks\n",
    "        remarks_df = data['remarks'][['text']]\n",
    "        # print('游remark_df:', remarks_df)\n",
    "        remarks_df.columns = ['remark']\n",
    "        remarks_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # Write each table starting from a new column\n",
    "        start_col = len(remarks_df.columns) + 2  # Adding 2 for separation\n",
    "        for i, (table_df, bbox, page) in enumerate(data['tables']):\n",
    "            table_df.to_excel(writer, sheet_name=combined_sheet_name, startrow=0, startcol=start_col, index=False)\n",
    "            # Increment start_col by the number of columns in the table + 2 for separation\n",
    "            start_col += len(table_df.columns) + 2\n",
    "\n",
    "print(\"Excel file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/scraping_base_with_needed_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "# with open('json_files/scraping_base_with_tables.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)\n",
    "\n",
    "scraping_base_bricolage = {k: v for k, v in list(scraping_base.items()) if k != 'Palladium' and k != 'Zirconium'}\n",
    "\n",
    "output_pdf_path = 'pdf_output/mcs2024_data_prod_reserve_tables.pdf'\n",
    "\n",
    "draw_rectangles_for_materials(pdf_path, output_pdf_path, scraping_base_bricolage)\n",
    "\n",
    "print(f\"Rectangles drawn for materials in {pdf_path} and saved to {output_pdf_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remark interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "pdf_path = \"mcs2024.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_input = 'excel_output\\production_reserve_tables1.xlsx'\n",
    "\n",
    "import json\n",
    "\n",
    "with open('json_files/scraping_base.json', 'r', encoding='utf-8') as json_file:\n",
    "    loaded_scraping_base = json.load(json_file)\n",
    "\n",
    "scraping_base = convert_dict_to_df(loaded_scraping_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_remarks = {material: {'remarks': content['remarks'][['text']]} for material, content in scraping_base.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the DataFrames in the dictionary\n",
    "# serialize_dict(material_remarks)\n",
    "\n",
    "# # Save the transformed dictionary as JSON\n",
    "# with open('materials_data.json', 'w') as json_file:\n",
    "#     json.dump(material_remarks, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_conversion_df = pd.read_csv('sub_materials_database.csv\\sub_materials_database_metric_conversion_factor.csv')\n",
    "\n",
    "chem_compo_df = pd.read_csv('sub_materials_database.csv\\sub_materials_database_sub_materials_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversion_factor(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'Not applicable' in value:\n",
    "            return value\n",
    "        # Extract numeric values using regex\n",
    "        match = re.search(r'([-+]?\\d*\\.\\d+|\\d+)', value)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "    return value\n",
    "\n",
    "metric_conversion_df['conversion_factor'] = metric_conversion_df['conversion_factor'].apply(process_conversion_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Mentioned</th>\n",
       "      <th>conversion_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Carat (diamond)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flask (fl)</td>\n",
       "      <td>0.03447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karat (gold)</td>\n",
       "      <td>Not applicable (a fraction, not a weight)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Long ton (lt)</td>\n",
       "      <td>101605.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Long ton unit (ltu)</td>\n",
       "      <td>0.01124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric Mentioned                          conversion_factor\n",
       "5      Carat (diamond)                                        0.0\n",
       "6           Flask (fl)                                    0.03447\n",
       "7         Karat (gold)  Not applicable (a fraction, not a weight)\n",
       "8        Long ton (lt)                                   101605.0\n",
       "9  Long ton unit (ltu)                                    0.01124"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_conversion_df[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set all 'final_unit' values to 't'\n",
    "metric_conversion_df['final_unit'] = 't'\n",
    "\n",
    "# Then, update 'final_unit' to 'karat' only for 'Karat (gold)'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Karat (gold)', 'final_unit'] = 'karat'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Mcf (1,000 cubic feet)', 'final_unit'] = 'cubic feet'\n",
    "metric_conversion_df.loc[metric_conversion_df['Metric Mentioned'] == 'Psia', 'final_unit'] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric Mentioned</th>\n",
       "      <th>conversion_factor</th>\n",
       "      <th>final_unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kilogram</td>\n",
       "      <td>0.001</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thousand Metric Tons</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metric Tons</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kilograms</td>\n",
       "      <td>0.001</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Million Metric Tons</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric Mentioned conversion_factor final_unit\n",
       "0              Kilogram             0.001          t\n",
       "1  Thousand Metric Tons            1000.0          t\n",
       "2           Metric Tons               1.0          t\n",
       "3             Kilograms             0.001          t\n",
       "4   Million Metric Tons         1000000.0          t"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_conversion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Copy the original Excel file to create an updated version\n",
    "excel_output = 'excel_output/produc_reserve_remarkinterpre.xlsx'\n",
    "shutil.copyfile(excel_input, excel_output)\n",
    "\n",
    "# Open the Excel file for updating\n",
    "with pd.ExcelWriter(excel_output, mode='a', if_sheet_exists='replace') as writer:\n",
    "    for material in material_remarks.keys():\n",
    "        # Convert the remarks to a DataFrame\n",
    "        material_sample = pd.DataFrame(material_remarks[material]['remarks'], columns=['text'])\n",
    "\n",
    "        # List of sub-material names from the reference_db\n",
    "        materials_list = chem_compo_df['sub_material_name'].tolist()\n",
    "\n",
    "        # List of metric units from the metric_conversion_df\n",
    "        metrics_list = metric_conversion_df['Metric Mentioned'].tolist()\n",
    "\n",
    "        # Apply the functions to the 'text' column of material_sample\n",
    "        material_sample['chemical_composition'] = material_sample['text'].apply(\n",
    "            lambda text: find_all_chemical_compositions(text, materials_list, chem_compo_df))\n",
    "        material_sample['metric_conv_factor'] = material_sample['text'].apply(\n",
    "            lambda text: find_metric_conversion_factor(text, metrics_list, metric_conversion_df))\n",
    "\n",
    "        # Read the corresponding sheet from the original Excel file\n",
    "        sheet_name = f\"{material}_raw_info\"\n",
    "        original_df = pd.read_excel(excel_input, sheet_name=sheet_name)\n",
    "\n",
    "        # Ensure the new DataFrame (material_sample) has the same index as the original_df\n",
    "        material_sample = material_sample.reindex(original_df.index)\n",
    "\n",
    "        # Insert the new columns next to the 'remark' column\n",
    "        original_df.insert(1, 'chemical_composition', material_sample['chemical_composition'])\n",
    "        original_df.insert(2, 'metric_conv_factor', material_sample['metric_conv_factor'])\n",
    "\n",
    "        # Write the updated sheet back to the Excel file\n",
    "        original_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## production of each material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_input = 'excel_output/produc_reserve_remarkinterpre.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = pd.ExcelFile(excel_input)\n",
    "\n",
    "# List to store dictionaries of sheet names and DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through all sheet names\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    \n",
    "    # Create a dictionary with sheet name and DataFrame\n",
    "    sheet_dict = {\n",
    "        f'sheet_name': sheet_name,\n",
    "        f'df': df\n",
    "    }\n",
    "    \n",
    "    # Append the dictionary to the list\n",
    "    dfs.append(sheet_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure the new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_exclude = ['Reserves', 'Capacity', 'reserves', 'capacity']  # Add or remove elements as needed\n",
    "\n",
    "def contains_excluded_word(column, excluded_words):\n",
    "    def check_cell(cell):\n",
    "        cell_str = str(cell).lower()\n",
    "        return any(word.lower() in cell_str for word in excluded_words)\n",
    "    \n",
    "    return column.apply(check_cell).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHEET NAME :  Abrasives_raw_info\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: C_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Fluorspar_raw_info\n",
      "Created/Updated column: Ca_2022.0\n",
      "Created/Updated column: F_2022.0\n",
      "Created/Updated column: Ca_2023.0\n",
      "Created/Updated column: F_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Mercury_raw_info\n",
      "Created/Updated column: Hg_2022.0\n",
      "Created/Updated column: Hg_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Silicon_raw_info\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: Mg_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Mg_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Aluminum_raw_info\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Gallium_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Mica_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Mica_2022\n",
      "Created/Updated column: Mica_2023\n",
      "游릭游릭游릭游릭游릭 ===>  Mica\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Silver_raw_info\n",
      "Created/Updated column: Ag_2022.0\n",
      "Created/Updated column: Ag_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Antimony_raw_info\n",
      "Created/Updated column: Sb_2022.0\n",
      "Created/Updated column: Sb_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Garnet_raw_info\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Mg_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Molybdenum_raw_info\n",
      "Created/Updated column: Mo_2022.0\n",
      "Created/Updated column: Mo_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Soda Ash_raw_info\n",
      "Created/Updated column: Na_2022.0\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Na_2023.0\n",
      "Created/Updated column: C_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Arsenic_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Arsenic_2022\n",
      "Created/Updated column: Arsenic_2023\n",
      "游릭游릭游릭游릭游릭 ===>  Arsenic\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Gemstones_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Gemstones_2022.0\n",
      "Created/Updated column: Gemstones_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Gemstones\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Nickel_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Stone_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Stone_2022\n",
      "Created/Updated column: Stone_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Stone\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Asbestos_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Germanium_raw_info\n",
      "No text columns remaining for sheet: Germanium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Niobium_raw_info\n",
      "Created/Updated column: Nb_2022.0\n",
      "Created/Updated column: Nb_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Strontium_raw_info\n",
      "Created/Updated column: Sr_2022.0\n",
      "Created/Updated column: Sr_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Barite_raw_info\n",
      "Created/Updated column: Ba_2022.0\n",
      "Created/Updated column: S_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Ba_2023.0\n",
      "Created/Updated column: S_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Gold_raw_info\n",
      "Created/Updated column: Au_2022.0\n",
      "Created/Updated column: Au_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Nitrogen_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Sulfur_raw_info\n",
      "Created/Updated column: S_2022.0\n",
      "Created/Updated column: S_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Bauxite_raw_info\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游댮 No valid chemical composition for Bauxite production. Keeping original data.\n",
      "游댮 No valid chemical composition for Bauxite production. Keeping original data.\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Graphite_raw_info\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: C_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Talc_raw_info\n",
      "Created/Updated column: Mg_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Mg_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Beryllium_raw_info\n",
      "Created/Updated column: Be_2022.0\n",
      "Created/Updated column: Be_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Gypsum_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Peat_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Peat_2022.0\n",
      "Created/Updated column: Peat_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Peat\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Tantalum_raw_info\n",
      "Created/Updated column: Ta_2022.0\n",
      "Created/Updated column: Ta_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Bismuth_raw_info\n",
      "Created/Updated column: Bi_2022.0\n",
      "Created/Updated column: Bi_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Hafnium_raw_info\n",
      "Created/Updated column: Zr_2022\n",
      "Created/Updated column: Si_2022\n",
      "Created/Updated column: O_2022\n",
      "Created/Updated column: Zr_2023\n",
      "Created/Updated column: Si_2023\n",
      "Created/Updated column: O_2023\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Perlite_raw_info\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Na_2022.0\n",
      "Created/Updated column: K_2022.0\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "Created/Updated column: Na_2023.0\n",
      "Created/Updated column: K_2023.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Tellurium_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Tellurium_2022.0\n",
      "Created/Updated column: Tellurium_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Tellurium\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Boron_raw_info\n",
      "Created/Updated column: B_2022.0\n",
      "Created/Updated column: B_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Helium_raw_info\n",
      "Created/Updated column: He_2022.0\n",
      "Created/Updated column: He_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Phosphate Rock_raw_info\n",
      "Created/Updated column: Ca_2022.0\n",
      "Created/Updated column: P_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: F_2022.0\n",
      "Created/Updated column: Ca_2023.0\n",
      "Created/Updated column: P_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "Created/Updated column: F_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Thallium_raw_info\n",
      "No text columns remaining for sheet: Thallium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Bromine_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Indium_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Indium_2022.0\n",
      "Created/Updated column: Indium_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Indium\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Platinum_raw_info\n",
      "Created/Updated column: Pt_2022\n",
      "Created/Updated column: Pt_2023\n",
      "Created/Updated column: Pt_2022\n",
      "Created/Updated column: Pt_2023\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Thorium_raw_info\n",
      "No text columns remaining for sheet: Thorium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Cadmium_raw_info\n",
      "Created/Updated column: Cd_2022.0\n",
      "Created/Updated column: Cd_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Iodine_raw_info\n",
      "Created/Updated column: I_2022.0\n",
      "Created/Updated column: I_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Potash_raw_info\n",
      "Created/Updated column: K_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: K_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Tin_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Cement_raw_info\n",
      "Created/Updated column: Ca_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Ca_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Iron and Steel_raw_info\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Mn_2022.0\n",
      "Created/Updated column: S_2022.0\n",
      "Created/Updated column: P_2022.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: C_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Mn_2023.0\n",
      "Created/Updated column: S_2023.0\n",
      "Created/Updated column: P_2023.0\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: Mn_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: C_2023.0\n",
      "Created/Updated column: Mn_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Pumice_raw_info\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Na_2022.0\n",
      "Created/Updated column: K_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "Created/Updated column: Na_2023.0\n",
      "Created/Updated column: K_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Titanium mineral_raw_info\n",
      "Created/Updated column: Zr_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Zr_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Titanium_raw_info\n",
      "Created/Updated column: Ti_2022.0\n",
      "Created/Updated column: Ti_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Cesium_raw_info\n",
      "No text columns remaining for sheet: Cesium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Iron Ore_raw_info\n",
      "Created/Updated column: Fe_2022\n",
      "Created/Updated column: O_2022\n",
      "Created/Updated column: Fe_2023\n",
      "Created/Updated column: Fe_2022\n",
      "Created/Updated column: Fe_2023\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Quartz_raw_info\n",
      "No text columns remaining for sheet: Quartz_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Tungsten_raw_info\n",
      "游 no match\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Chromium_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Iron Oxide Pigments_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Rare Earths_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Rare Earths_2022.0\n",
      "Created/Updated column: Rare Earths_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Rare Earths\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Vanadium_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Clays_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Clays_2022\n",
      "Created/Updated column: Clays_2023\n",
      "Created/Updated column: Clays_2022\n",
      "Created/Updated column: Clays_2023\n",
      "Created/Updated column: Clays_2022\n",
      "Created/Updated column: Clays_2023\n",
      "游릭游릭游릭游릭游릭 ===>  Clays\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Kyanite_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Rhenium_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Rhenium_2022.0\n",
      "Created/Updated column: Rhenium_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Rhenium\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Vermiculite_raw_info\n",
      "Created/Updated column: Mg_2022.0\n",
      "Created/Updated column: Fe_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Mg_2023.0\n",
      "Created/Updated column: Fe_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Cobalt_raw_info\n",
      "Created/Updated column: Co_2022.0\n",
      "Created/Updated column: Co_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Lead_raw_info\n",
      "Created/Updated column: Pb_2022.0\n",
      "Created/Updated column: Pb_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Rubidium_raw_info\n",
      "No text columns remaining for sheet: Rubidium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Wollastonite_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Copper_raw_info\n",
      "Created/Updated column: Cu_2022.0\n",
      "Created/Updated column: Cu_2023.0\n",
      "Created/Updated column: Cu_2022.0\n",
      "Created/Updated column: Cu_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Lime_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Salt_raw_info\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Yttrium_raw_info\n",
      "No text columns remaining for sheet: Yttrium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Diamond_raw_info\n",
      "Created/Updated column: C_2022.0\n",
      "Created/Updated column: C_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Lithium_raw_info\n",
      "Created/Updated column: Li_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Sand and Gravel_raw_info\n",
      "Created/Updated column: Si_2022\n",
      "Created/Updated column: O_2022\n",
      "Created/Updated column: Si_2023\n",
      "Created/Updated column: O_2023\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Zeolites_raw_info\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Na_2022.0\n",
      "Created/Updated column: H_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "Created/Updated column: Na_2023.0\n",
      "Created/Updated column: H_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Diatomite_raw_info\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Magnesium_raw_info\n",
      "Created/Updated column: Mg_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: Mg_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Scandium_raw_info\n",
      "No text columns remaining for sheet: Scandium_raw_info\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Zinc_raw_info\n",
      "Created/Updated column: Zn_2022.0\n",
      "Created/Updated column: Zn_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Feldspar_raw_info\n",
      "Created/Updated column: K_2022.0\n",
      "Created/Updated column: Na_2022.0\n",
      "Created/Updated column: Al_2022.0\n",
      "Created/Updated column: Si_2022.0\n",
      "Created/Updated column: O_2022.0\n",
      "Created/Updated column: K_2023.0\n",
      "Created/Updated column: Na_2023.0\n",
      "Created/Updated column: Al_2023.0\n",
      "Created/Updated column: Si_2023.0\n",
      "Created/Updated column: O_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Manganese_raw_info\n",
      "Created/Updated column: Mn_2022.0\n",
      "Created/Updated column: Mn_2023.0\n",
      "游릭 not varied\n",
      "==================================================\n",
      "\n",
      "SHEET NAME :  Selenium_raw_info\n",
      "游릭 varied\n",
      "Created/Updated column: Selenium_2022.0\n",
      "Created/Updated column: Selenium_2023.0\n",
      "游릭游릭游릭游릭游릭 ===>  Selenium\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "def match_composition(input_name, reference_df):\n",
    "    input_name = str(input_name).lower()\n",
    "    best_match = None\n",
    "    best_ratio = 0\n",
    "    for material in reference_df['sub_material_name']:\n",
    "        ratio = fuzz.partial_ratio(input_name, str(material).lower())\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = material\n",
    "    if best_ratio < 80:\n",
    "        return None\n",
    "    composition = reference_df.loc[reference_df['sub_material_name'] == best_match, 'chemical_composition'].iloc[0]\n",
    "    return composition\n",
    "\n",
    "def clean_numeric(val):\n",
    "    if isinstance(val, str):\n",
    "        cleaned = re.sub(r'[^\\d.]+', '', val)\n",
    "        return float(cleaned) if cleaned else np.nan\n",
    "    return val\n",
    "\n",
    "def convert_year_to_float(year):\n",
    "    if isinstance(year, str):\n",
    "        year = year.strip().lower()\n",
    "        if year.endswith('e'):\n",
    "            return float(year[:-1])\n",
    "        try:\n",
    "            return float(year)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return int(year) if pd.notnull(year) else np.nan\n",
    "\n",
    "def process_column(df, text_df, title_row, index, item, years):\n",
    "    for year in years:\n",
    "        year_float = convert_year_to_float(year)\n",
    "        if pd.notna(year_float):\n",
    "            chem_comp_dict = item['chem_comp']\n",
    "            if chem_comp_dict:\n",
    "                for elem, percentage in chem_comp_dict.items():\n",
    "                    new_col_name = f\"{elem}_{year_float}\"\n",
    "                    try:\n",
    "                        numeric_col = text_df.iloc[2:][title_row[index]].apply(clean_numeric)\n",
    "                        result = numeric_col * percentage\n",
    "                        result = result.replace([np.inf, -np.inf], np.nan)\n",
    "                        if new_col_name in df.columns:\n",
    "                            df[new_col_name] += result\n",
    "                        else:\n",
    "                            df[new_col_name] = pd.Series(index=df.index)\n",
    "                            df.loc[df.index[2:], new_col_name] = result.values\n",
    "                        print(f\"Created/Updated column: {new_col_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"游댮Error processing column {title_row[index]}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"游댮 No valid chemical composition for {item['material']}. Keeping original data.\")\n",
    "\n",
    "def process_varied_composition(df, text_df, title_row, sheet_material, first_row, second_row):\n",
    "    for index, col in enumerate(title_row):\n",
    "        if text_df[col].iloc[2:].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna().any():\n",
    "            years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "            for year in years:\n",
    "                year_float = convert_year_to_float(year)\n",
    "                if pd.notna(year_float):\n",
    "                    new_col_name = f\"{sheet_material}_{year_float}\"\n",
    "                    try:\n",
    "                        numeric_col = text_df.iloc[2:][col].apply(clean_numeric)\n",
    "                        if new_col_name in df.columns:\n",
    "                            df[new_col_name] += numeric_col\n",
    "                        else:\n",
    "                            df[new_col_name] = pd.Series(index=df.index)\n",
    "                            df.loc[df.index[2:], new_col_name] = numeric_col.values\n",
    "                        print(f\"Created/Updated column: {new_col_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"游댮Error processing column {col}: {str(e)}\")\n",
    "\n",
    "materials_missing_comp = []\n",
    "for i in range(0, len(dfs)):\n",
    "    sheet_name = dfs[i]['sheet_name']\n",
    "    df = dfs[i]['df'].copy()\n",
    "    print(\"SHEET NAME : \", sheet_name)\n",
    "\n",
    "    original_text_columns = [col for col in df.columns if col.startswith('text')]\n",
    "    columns_to_keep = [col for col in df.columns if not contains_excluded_word(df[col], words_to_exclude)]\n",
    "    df = df[columns_to_keep]\n",
    "    remaining_text_columns = [col for col in df.columns if col.startswith('text')]\n",
    "\n",
    "    if remaining_text_columns:\n",
    "        text_df = df[remaining_text_columns]\n",
    "        first_row = text_df.iloc[0] if not text_df.empty else pd.Series()\n",
    "        second_row = text_df.iloc[1] if len(text_df) > 1 else pd.Series()\n",
    "        title_row = list(text_df.columns)\n",
    "        materials = list(pd.unique(first_row.dropna().values))\n",
    "\n",
    "        list_chem_comp = []\n",
    "        for material in materials:\n",
    "            chem_comp = match_composition(material, chem_compo_df)\n",
    "            list_chem_comp.append({\"material\": material, \"chem_comp\": chem_comp})\n",
    "\n",
    "        if all(item['chem_comp'] is None for item in list_chem_comp):\n",
    "            material = sheet_name.replace('_raw_info', '')\n",
    "            list_chem_comp = [{\"material\": material, \"chem_comp\": match_composition(material, chem_compo_df)}]\n",
    "\n",
    "        if all(item['chem_comp'] is None for item in list_chem_comp):\n",
    "            materials_missing_comp.append(sheet_name.replace('_raw_info', ''))\n",
    "            print(\"游 no match\")\n",
    "        else:\n",
    "            list_chem_comp = convert_string_to_dict(list_chem_comp)\n",
    "            df['chemical_composition'] = str(list_chem_comp)\n",
    "\n",
    "            if list_chem_comp[0]['chem_comp'] != 'Varied':\n",
    "                sheet_material = sheet_name.replace('_raw_info', '')\n",
    "                is_sheet_material = any(item['material'] == sheet_material for item in list_chem_comp)\n",
    "\n",
    "                if is_sheet_material:\n",
    "                    for index, col in enumerate(title_row):\n",
    "                        if text_df[col].iloc[2:].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna().any():\n",
    "                            item = next((item for item in list_chem_comp if item['material'] == sheet_material), None)\n",
    "                            if item:\n",
    "                                years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "                                process_column(df, text_df, title_row, index, item, years)\n",
    "                else:\n",
    "                    for index, col in enumerate(first_row):\n",
    "                        for item in list_chem_comp:\n",
    "                            if col == item['material']:\n",
    "                                years = second_row.iloc[index].split(',') if isinstance(second_row.iloc[index], str) else [second_row.iloc[index]]\n",
    "                                process_column(df, text_df, title_row, index, item, years)\n",
    "                print('游릭 not varied')\n",
    "            else:\n",
    "                print('游릭 varied')\n",
    "                sheet_material = sheet_name.replace('_raw_info', '')\n",
    "                process_varied_composition(df, text_df, title_row, sheet_material, first_row, second_row)\n",
    "                print(\"游릭游릭游릭游릭游릭 ===> \", sheet_material)\n",
    "    else:\n",
    "        print(f\"No text columns remaining for sheet: {sheet_name}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    dfs[i]['df'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tungsten']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "materials_missing_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed DataFrames have been saved to excel_output/production_each_material.xlsx with sheets sorted alphabetically.\n"
     ]
    }
   ],
   "source": [
    "# Sort the dfs list by sheet_name\n",
    "dfs = sorted(dfs, key=lambda x: x['sheet_name'])\n",
    "\n",
    "# Save all DataFrames to a single Excel file\n",
    "output_file = 'excel_output/production_each_material.xlsx'\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for df_dict in dfs:\n",
    "        sheet_name = df_dict['sheet_name']\n",
    "        df = df_dict['df']\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"All processed DataFrames have been saved to {output_file} with sheets sorted alphabetically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_post_text_columns(columns):\n",
    "#     # Find the index of the last column starting with 'text'\n",
    "#     last_text_index = max([i for i, col in enumerate(columns) if col.startswith('text')], default=-1)\n",
    "    \n",
    "#     # If no 'text' columns found, return an empty list\n",
    "#     if last_text_index == -1:\n",
    "#         return []\n",
    "    \n",
    "#     # Return all columns after the last 'text' column\n",
    "#     return columns[last_text_index + 1:]\n",
    "\n",
    "\n",
    "# for i in range(0, len(dfs)-13):\n",
    "#     sheet_name = dfs[i]['sheet_name']\n",
    "#     df = dfs[i]['df'].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "#     print(\"SHEET NAME : \", sheet_name)\n",
    "#     # Safely access conv_factor\n",
    "#     conv_factor = df['metric_conv_factor'].iloc[0] if 'metric_conv_factor' in df.columns else None\n",
    "#     cols = list(df.columns)\n",
    "#     post_text_columns = detect_post_text_columns(cols)\n",
    "\n",
    "#     if post_text_columns:\n",
    "#         data = df[['text']+post_text_columns]\n",
    "#         data = data.dropna(how='all')\n",
    "\n",
    "#         if conv_factor is not string:\n",
    "#             multiply all numercal values with conversion factor\n",
    "        \n",
    "#         res_dfs.append(data)\n",
    "    \n",
    "#     merge all data(s) in res_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHEET NAME :  Abrasives_raw_info\n",
      "Applied conversion factor nan to numerical values in Abrasives_raw_info\n",
      "SHEET NAME :  Aluminum_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Aluminum_raw_info\n",
      "SHEET NAME :  Antimony_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Antimony_raw_info\n",
      "SHEET NAME :  Arsenic_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Arsenic_raw_info\n",
      "SHEET NAME :  Asbestos_raw_info\n",
      "No post-text columns found in Asbestos_raw_info\n",
      "SHEET NAME :  Barite_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Barite_raw_info\n",
      "SHEET NAME :  Bauxite_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Bauxite_raw_info\n",
      "SHEET NAME :  Beryllium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Beryllium_raw_info\n",
      "SHEET NAME :  Bismuth_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Bismuth_raw_info\n",
      "SHEET NAME :  Boron_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Boron_raw_info\n",
      "SHEET NAME :  Bromine_raw_info\n",
      "No post-text columns found in Bromine_raw_info\n",
      "SHEET NAME :  Cadmium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Cadmium_raw_info\n",
      "SHEET NAME :  Cement_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Cement_raw_info\n",
      "SHEET NAME :  Cesium_raw_info\n",
      "No post-text columns found in Cesium_raw_info\n",
      "SHEET NAME :  Chromium_raw_info\n",
      "No post-text columns found in Chromium_raw_info\n",
      "SHEET NAME :  Clays_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Clays_raw_info\n",
      "SHEET NAME :  Cobalt_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Cobalt_raw_info\n",
      "SHEET NAME :  Copper_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Copper_raw_info\n",
      "SHEET NAME :  Diamond_raw_info\n",
      "Applied conversion factor 0.2 to numerical values in Diamond_raw_info\n",
      "SHEET NAME :  Diatomite_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Diatomite_raw_info\n",
      "SHEET NAME :  Feldspar_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Feldspar_raw_info\n",
      "SHEET NAME :  Fluorspar_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Fluorspar_raw_info\n",
      "SHEET NAME :  Gallium_raw_info\n",
      "No post-text columns found in Gallium_raw_info\n",
      "SHEET NAME :  Garnet_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Garnet_raw_info\n",
      "SHEET NAME :  Gemstones_raw_info\n",
      "Applied conversion factor nan to numerical values in Gemstones_raw_info\n",
      "SHEET NAME :  Germanium_raw_info\n",
      "No post-text columns found in Germanium_raw_info\n",
      "SHEET NAME :  Gold_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Gold_raw_info\n",
      "SHEET NAME :  Graphite_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Graphite_raw_info\n",
      "SHEET NAME :  Gypsum_raw_info\n",
      "No post-text columns found in Gypsum_raw_info\n",
      "SHEET NAME :  Hafnium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Hafnium_raw_info\n",
      "SHEET NAME :  Helium_raw_info\n",
      "Applied conversion factor nan to numerical values in Helium_raw_info\n",
      "SHEET NAME :  Indium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Indium_raw_info\n",
      "SHEET NAME :  Iodine_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Iodine_raw_info\n",
      "SHEET NAME :  Iron Ore_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Iron Ore_raw_info\n",
      "SHEET NAME :  Iron Oxide Pigments_raw_info\n",
      "No post-text columns found in Iron Oxide Pigments_raw_info\n",
      "SHEET NAME :  Iron and Steel_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Iron and Steel_raw_info\n",
      "SHEET NAME :  Kyanite_raw_info\n",
      "No post-text columns found in Kyanite_raw_info\n",
      "SHEET NAME :  Lead_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Lead_raw_info\n",
      "SHEET NAME :  Lime_raw_info\n",
      "No post-text columns found in Lime_raw_info\n",
      "SHEET NAME :  Lithium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Lithium_raw_info\n",
      "SHEET NAME :  Magnesium_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Magnesium_raw_info\n",
      "SHEET NAME :  Manganese_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Manganese_raw_info\n",
      "SHEET NAME :  Mercury_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Mercury_raw_info\n",
      "SHEET NAME :  Mica_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Mica_raw_info\n",
      "SHEET NAME :  Molybdenum_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Molybdenum_raw_info\n",
      "SHEET NAME :  Nickel_raw_info\n",
      "No post-text columns found in Nickel_raw_info\n",
      "SHEET NAME :  Niobium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Niobium_raw_info\n",
      "SHEET NAME :  Nitrogen_raw_info\n",
      "No post-text columns found in Nitrogen_raw_info\n",
      "SHEET NAME :  Peat_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Peat_raw_info\n",
      "SHEET NAME :  Perlite_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Perlite_raw_info\n",
      "SHEET NAME :  Phosphate Rock_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Phosphate Rock_raw_info\n",
      "SHEET NAME :  Platinum_raw_info\n",
      "Applied conversion factor nan to numerical values in Platinum_raw_info\n",
      "SHEET NAME :  Potash_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Potash_raw_info\n",
      "SHEET NAME :  Pumice_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Pumice_raw_info\n",
      "SHEET NAME :  Quartz_raw_info\n",
      "No post-text columns found in Quartz_raw_info\n",
      "SHEET NAME :  Rare Earths_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Rare Earths_raw_info\n",
      "SHEET NAME :  Rhenium_raw_info\n",
      "Applied conversion factor 0.001 to numerical values in Rhenium_raw_info\n",
      "SHEET NAME :  Rubidium_raw_info\n",
      "No post-text columns found in Rubidium_raw_info\n",
      "SHEET NAME :  Salt_raw_info\n",
      "No post-text columns found in Salt_raw_info\n",
      "SHEET NAME :  Sand and Gravel_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Sand and Gravel_raw_info\n",
      "SHEET NAME :  Scandium_raw_info\n",
      "No post-text columns found in Scandium_raw_info\n",
      "SHEET NAME :  Selenium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Selenium_raw_info\n",
      "SHEET NAME :  Silicon_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Silicon_raw_info\n",
      "SHEET NAME :  Silver_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Silver_raw_info\n",
      "SHEET NAME :  Soda Ash_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Soda Ash_raw_info\n",
      "SHEET NAME :  Stone_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Stone_raw_info\n",
      "SHEET NAME :  Strontium_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Strontium_raw_info\n",
      "SHEET NAME :  Sulfur_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Sulfur_raw_info\n",
      "SHEET NAME :  Talc_raw_info\n",
      "Applied conversion factor 1000.0 to numerical values in Talc_raw_info\n",
      "SHEET NAME :  Tantalum_raw_info\n",
      "Applied conversion factor 1.0 to numerical values in Tantalum_raw_info\n",
      "All dataframes merged successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_post_text_columns(columns):\n",
    "    last_text_index = max([i for i, col in enumerate(columns) if col.startswith('text')], default=-1)\n",
    "    return columns[last_text_index + 1:] if last_text_index != -1 else []\n",
    "\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "res_dfs = []\n",
    "\n",
    "for i in range(0, len(dfs)-13):\n",
    "    sheet_name = dfs[i]['sheet_name']\n",
    "    df = dfs[i]['df'].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    print(\"SHEET NAME : \", sheet_name)\n",
    "    \n",
    "    # Safely access conv_factor\n",
    "    conv_factor = df['metric_conv_factor'].iloc[0] if 'metric_conv_factor' in df.columns else None\n",
    "    cols = list(df.columns)\n",
    "    post_text_columns = detect_post_text_columns(cols)\n",
    "\n",
    "    if post_text_columns:\n",
    "        data = df[['text'] + post_text_columns]\n",
    "        data = data.dropna(how='all')\n",
    "\n",
    "        if conv_factor is not None and not isinstance(conv_factor, str):\n",
    "            try:\n",
    "                conv_factor = float(conv_factor)\n",
    "                for col in post_text_columns:\n",
    "                    data[col] = data[col].apply(lambda x: x * conv_factor if is_numeric(x) else x)\n",
    "                print(f\"Applied conversion factor {conv_factor} to numerical values in {sheet_name}\")\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not convert {conv_factor} to float in {sheet_name}\")\n",
    "        \n",
    "        res_dfs.append(data)\n",
    "    else:\n",
    "        print(f\"No post-text columns found in {sheet_name}\")\n",
    "\n",
    "# Merge all data(s) in res_dfs\n",
    "if res_dfs:\n",
    "    merged_df = pd.concat(res_dfs, keys=[f\"Sheet_{i}\" for i in range(len(res_dfs))], axis=0)\n",
    "    print(\"All dataframes merged successfully\")\n",
    "else:\n",
    "    merged_df = pd.DataFrame()\n",
    "    print(\"No dataframes to merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 94)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Al_2022.0', 'O_2022.0', 'Al_2023.0', 'O_2023.0', 'Si_2022.0',\n",
       "       'C_2022.0', 'Si_2023.0', 'C_2023.0', 'Sb_2022.0', 'Sb_2023.0',\n",
       "       'Arsenic_2022', 'Arsenic_2023', 'Ba_2022.0', 'S_2022.0', 'Ba_2023.0',\n",
       "       'S_2023.0', 'Be_2022.0', 'Be_2023.0', 'Bi_2022.0', 'Bi_2023.0',\n",
       "       'B_2022.0', 'B_2023.0', 'Cd_2022.0', 'Cd_2023.0', 'Ca_2022.0',\n",
       "       'Fe_2022.0', 'Ca_2023.0', 'Fe_2023.0', 'Clays_2022', 'Clays_2023',\n",
       "       'Co_2022.0', 'Co_2023.0', 'Cu_2022.0', 'Cu_2023.0', 'K_2022.0',\n",
       "       'Na_2022.0', 'K_2023.0', 'Na_2023.0', 'F_2022.0', 'F_2023.0',\n",
       "       'Mg_2023.0', 'Gemstones_2022.0', 'Gemstones_2023.0', 'Au_2022.0',\n",
       "       'Au_2023.0', 'Zr_2022', 'Si_2022', 'O_2022', 'Zr_2023', 'Si_2023',\n",
       "       'O_2023', 'He_2022.0', 'He_2023.0', 'Indium_2022.0', 'Indium_2023.0',\n",
       "       'I_2022.0', 'I_2023.0', 'Fe_2022', 'Fe_2023', 'Mn_2022.0', 'P_2022.0',\n",
       "       'Mn_2023.0', 'P_2023.0', 'Pb_2022.0', 'Pb_2023.0', 'Li_2023.0',\n",
       "       'Mg_2022.0', 'Hg_2022.0', 'Hg_2023.0', 'Mica_2022', 'Mica_2023',\n",
       "       'Mo_2022.0', 'Mo_2023.0', 'Nb_2022.0', 'Nb_2023.0', 'Peat_2022.0',\n",
       "       'Peat_2023.0', 'Pt_2022', 'Pt_2023', 'Rare Earths_2022.0',\n",
       "       'Rare Earths_2023.0', 'Rhenium_2022.0', 'Rhenium_2023.0',\n",
       "       'Selenium_2022.0', 'Selenium_2023.0', 'Ag_2022.0', 'Ag_2023.0',\n",
       "       'Stone_2022', 'Stone_2023.0', 'Sr_2022.0', 'Sr_2023.0', 'Ta_2022.0',\n",
       "       'Ta_2023.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns = merged_df.columns.str.replace(r'\\.0$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text'] = merged_df['text'].str.lower()\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First replace 'world total' with a common string\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'^world total.*', 'world total', regex=True, case=False)\n",
    "\n",
    "# Remove any text between parentheses and strip leading/trailing spaces\n",
    "merged_df['text'] = merged_df['text'].str.replace(r'\\(.*?\\)', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df[merged_df['text'].str.startswith('world total')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text'] = merged_df['text'].str.split(',', n=1).str[0].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = ['natural:', 'other', 'other countries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df['text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_summed = merged_df.groupby('text', as_index=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DataScience\\AppData\\Local\\Temp\\ipykernel_40968\\3528072658.py:1: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  out_df = merged_df_summed.groupby(merged_df_summed.columns, axis=1).sum()\n"
     ]
    }
   ],
   "source": [
    "out_df = merged_df_summed.groupby(merged_df_summed.columns, axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the 'world total' row\n",
    "# world_total_row = out_df[out_df['text'] == 'world total']\n",
    "# other_rows = out_df[out_df['text'] != 'world total']\n",
    "\n",
    "# # Calculate the sum of all country rows\n",
    "# calculated_world_total = other_rows.select_dtypes(include=[np.number]).sum().to_frame().T\n",
    "# calculated_world_total['text'] = 'calculated world total'\n",
    "\n",
    "# # Combine the rows\n",
    "# out_df = pd.concat([other_rows, calculated_world_total], ignore_index=True)\n",
    "\n",
    "# # Sort the columns to ensure they are in the correct order\n",
    "# out_df = out_df.reindex(columns=['text'] + sorted([col for col in out_df.columns if col != 'text']))\n",
    "\n",
    "# # Reset the index\n",
    "# out_df = out_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = ['natural:', 'other', 'other countries']\n",
    "\n",
    "# Remove rows where 'text' equals any value in the outliers list\n",
    "merged_df = merged_df[~merged_df['text'].isin(outliers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# List of standard country names\n",
    "countries = [\n",
    "    \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\", \"Argentina\", \"Armenia\", \"Australia\", \n",
    "    \"Austria\", \"Azerbaijan\", \"Bahamas\", \"Bahrain\", \"Bangladesh\", \"Barbados\", \"Belarus\", \"Belgium\", \"Belize\", \"Benin\", \n",
    "    \"Bhutan\", \"Bolivia\", \"Bosnia and Herzegovina\", \"Botswana\", \"Brazil\", \"Brunei\", \"Bulgaria\", \"Burkina Faso\", \"Burundi\", \n",
    "    \"Cabo Verde\", \"Cambodia\", \"Cameroon\", \"Canada\", \"Central African Republic\", \"Chad\", \"Chile\", \"China\", \"Colombia\", \n",
    "    \"Comoros\", \"Congo\", \"Costa Rica\", \"Croatia\", \"Cuba\", \"Cyprus\", \"Czech Republic\", \"Democratic Republic of the Congo\", \n",
    "    \"Denmark\", \"Djibouti\", \"Dominica\", \"Dominican Republic\", \"Ecuador\", \"Egypt\", \"El Salvador\", \"Equatorial Guinea\", \n",
    "    \"Eritrea\", \"Estonia\", \"Eswatini\", \"Ethiopia\", \"Fiji\", \"Finland\", \"France\", \"Gabon\", \"Gambia\", \"Georgia\", \"Germany\", \n",
    "    \"Ghana\", \"Greece\", \"Grenada\", \"Guatemala\", \"Guinea\", \"Guinea-Bissau\", \"Guyana\", \"Haiti\", \"Honduras\", \"Hungary\", \n",
    "    \"Iceland\", \"India\", \"Indonesia\", \"Iran\", \"Iraq\", \"Ireland\", \"Israel\", \"Italy\", \"Ivory Coast\", \"Jamaica\", \"Japan\", \n",
    "    \"Jordan\", \"Kazakhstan\", \"Kenya\", \"Kiribati\", \"Kuwait\", \"Kyrgyzstan\", \"Laos\", \"Latvia\", \"Lebanon\", \"Lesotho\", \"Liberia\", \n",
    "    \"Libya\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Madagascar\", \"Malawi\", \"Malaysia\", \"Maldives\", \"Mali\", \"Malta\", \n",
    "    \"Marshall Islands\", \"Mauritania\", \"Mauritius\", \"Mexico\", \"Micronesia\", \"Moldova\", \"Monaco\", \"Mongolia\", \"Montenegro\", \n",
    "    \"Morocco\", \"Mozambique\", \"Myanmar\", \"Namibia\", \"Nauru\", \"Nepal\", \"Netherlands\", \"New Zealand\", \"Nicaragua\", \"Niger\", \n",
    "    \"Nigeria\", \"North Korea\", \"North Macedonia\", \"Norway\", \"Oman\", \"Pakistan\", \"Palau\", \"Palestine\", \"Panama\", \n",
    "    \"Papua New Guinea\", \"Paraguay\", \"Peru\", \"Philippines\", \"Poland\", \"Portugal\", \"Qatar\", \"Romania\", \"Russia\", \"Rwanda\", \n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\", \"Samoa\", \"San Marino\", \n",
    "    \"Sao Tome and Principe\", \"Saudi Arabia\", \"Senegal\", \"Serbia\", \"Seychelles\", \"Sierra Leone\", \"Singapore\", \"Slovakia\", \n",
    "    \"Slovenia\", \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Korea\", \"South Sudan\", \"Spain\", \"Sri Lanka\", \"Sudan\", \n",
    "    \"Suriname\", \"Sweden\", \"Switzerland\", \"Syria\", \"Taiwan\", \"Tajikistan\", \"Tanzania\", \"Thailand\", \"Timor-Leste\", \"Togo\", \n",
    "    \"Tonga\", \"Trinidad and Tobago\", \"Tunisia\", \"Turkey\", \"Turkmenistan\", \"Tuvalu\", \"Uganda\", \"Ukraine\", \n",
    "    \"United Arab Emirates\", \"United Kingdom\", \"United States\", \"Uruguay\", \"Uzbekistan\", \"Vanuatu\", \"Vatican City\", \n",
    "    \"Venezuela\", \"Vietnam\", \"Yemen\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "def match_country(country, choices, cutoff=80):\n",
    "    match = process.extractOne(country, choices, score_cutoff=cutoff)\n",
    "    return match[0] if match else country\n",
    "\n",
    "# Apply the matching function to the 'text' column\n",
    "out_df['text'] = out_df['text'].apply(lambda x: match_country(x, countries) if x != 'world total' and x != 'calculated world total' else x)\n",
    "\n",
    "# Group by the standardized names and sum the numeric columns\n",
    "out_df = out_df.groupby('text', as_index=False).sum()\n",
    "\n",
    "# Recalculate the world total\n",
    "world_total_row = out_df[out_df['text'] == 'world total']\n",
    "other_rows = out_df[out_df['text'] != 'world total']\n",
    "\n",
    "calculated_world_total = other_rows.select_dtypes(include=[np.number]).sum().to_frame().T\n",
    "calculated_world_total['text'] = 'calculated world total'\n",
    "\n",
    "# Combine the rows\n",
    "out_df = pd.concat([other_rows, calculated_world_total], ignore_index=True)\n",
    "\n",
    "# Sort the columns to ensure they are in the correct order\n",
    "out_df = out_df.reindex(columns=['text'] + sorted([col for col in out_df.columns if col != 'text']))\n",
    "\n",
    "# Reset the index\n",
    "out_df = out_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the columns list with 'text' at the beginning\n",
    "columns_order = ['text'] + [col for col in out_df.columns if col != 'text']\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "out_df = out_df[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out_df into 2022_df and 2023_df\n",
    "columns_2022 = ['text'] + [col for col in out_df.columns if col.endswith('2022')]\n",
    "columns_2023 = ['text'] + [col for col in out_df.columns if col.endswith('2023')]\n",
    "\n",
    "# Create DataFrames for each year\n",
    "df_22 = out_df[columns_2022]\n",
    "df_23 = out_df[columns_2023]\n",
    "\n",
    "# Save to Excel with two sheets\n",
    "with pd.ExcelWriter('final_table.xlsx', engine='openpyxl') as writer:\n",
    "    df_22.to_excel(writer, sheet_name='2022 Data', index=False)\n",
    "    df_23.to_excel(writer, sheet_name='2023 Data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvRi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
